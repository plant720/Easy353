#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time       : 2022/4/6 2:59 下午
# @Author     : zzhen
# @File       : filter_1.py
# @Software   : PyCharm
# @Description: 
# @Copyright  : Copyright (c) 2022 by sculab, All Rights Reserved.
import collections
import gzip
import multiprocessing
import os
import shutil
import time
import gc
import platform
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial
import psutil
import argparse
# import my python file
from Easy353Lib.utils import bytes_str, reverse_complement_limit, make_ref_kmer_dict, get_file_size, get_ref_info, \
    get_reads_info, log


# Determine which reference file has the kmer generated by read,
# Returns the file name/gene name, os.path.basename(file_path).split('.')[0]
# ref_hash_dict is the dict of the reference sequences
def filter_read_for_gene(_ref_kmer_dict_: dict, _kmer_size_: int, _step_size_: int, _read_seq_: str,
                         _read_reverse_complement_: bool = True) -> set:
    ref_gene_name_list = []
    # have \n, not need len-kmer_size+1
    for j in range(0, len(_read_seq_) - _kmer_size_, _step_size_):
        read_kmer = _read_seq_[j:j + _kmer_size_]
        if read_kmer in _ref_kmer_dict_:
            ref_gene_name_list.extend(_ref_kmer_dict_.get(read_kmer)[2:])
    if _read_reverse_complement_:
        _read_seq_ = reverse_complement_limit(_read_seq_)
        for j in range(0, len(_read_seq_) - _kmer_size_, _step_size_):
            read_kmer = _read_seq_[j:j + _kmer_size_]
            if read_kmer in _ref_kmer_dict_:
                ref_gene_name_list.extend(_ref_kmer_dict_.get(read_kmer)[2:])
    return set(ref_gene_name_list)


# _ref_hash_dict_ s a hash table constructed from the reference sequences
# _step_size_ the length of the sliding window on the reads
def do_reads_filter(_ref_kmer_dict_: dict, _kmer_size_: int, _step_size_: int, _file_path_: str, _out_dir_: str,
                    _read_reverse_complement_, _print_=True) -> None:
    _time_start_, _time_end_, reads_count = time.time(), 0, 0
    # detect whether  file is gzipped or not
    bytes_type = _file_path_[-3:].lower() == ".gz"
    infile_stream = gzip.open(_file_path_, 'rb') if bytes_type else open(_file_path_, 'rb')
    for _ in infile_stream:
        reads_count += 1
        temp_rec = [_, infile_stream.readline(), infile_stream.readline(), infile_stream.readline()]
        for file_name in filter_read_for_gene(_ref_kmer_dict_, _kmer_size_, _step_size_,
                                              bytes_str(temp_rec[1]),
                                              _read_reverse_complement_):
            with open(os.path.join(_out_dir_, file_name + ".fasta"), "a+") as outfile:
                outfile.writelines(['>', bytes_str(temp_rec[0]), bytes_str(temp_rec[1])])
        if reads_count % 1000000 == 0:
            _time_end_ = time.time()
            _time_start_, _time_end_ = _time_end_, _time_end_ - _time_start_
            if _print_:
                print('handled\t', reads_count // 1000000, 'm reads, ', round(_time_end_, 2), 's/m reads', sep="",
                      end='\r')
    infile_stream.close()


# For memory reason, ref_reverse_complement=False,so _read_reverse_complement_=True
# t_id and t_count is used for multiprocessing
def do_pair_reads_filter(_ref_kmer_dict_: dict, _kmer_size_: int, _step_size_: int, fq_file_1: str,
                         fq_file_2: str, _out_dir_: str, t_id: int, t_count: int,
                         _read_reverse_complement_: bool, _print_=True) -> None:
    _time_start_, _time_end_, reads_count = time.time(), 0, 0
    bytes_type = fq_file_1[-3:].lower() == ".gz"
    if fq_file_1 == fq_file_2:
        print("Error: the paired reads file is same!")
        exit(1)
    infile_1 = gzip.open(fq_file_1, 'rb') if bytes_type else open(fq_file_1, 'rb')
    infile_2 = gzip.open(fq_file_2, 'rb') if bytes_type else open(fq_file_2, 'rb')
    for _ in infile_1:
        reads_count += 1
        temp_rec1 = [_, infile_1.readline(), infile_1.readline(), infile_1.readline()]
        temp_rec2 = [infile_2.readline(), infile_2.readline(), infile_2.readline(), infile_2.readline()]
        if reads_count % t_count == t_id:
            # write kmers into the files
            for file_name in filter_read_for_gene(_ref_kmer_dict_, _kmer_size_, _step_size_,
                                                  bytes_str(temp_rec1[1]),
                                                  _read_reverse_complement_):
                with open(os.path.join(_out_dir_, file_name + ".fasta"), "a+") as _out_file:
                    _out_file.writelines(['>', bytes_str(temp_rec1[0]), bytes_str(temp_rec1[1])])
                    _out_file.writelines(['>', bytes_str(temp_rec2[0]), bytes_str(temp_rec2[1])])
        if reads_count * 2 % 1000000 == 0:
            _time_end_ = time.time()
            _time_start_, _time_end_ = _time_end_, _time_end_ - _time_start_
            if _print_:
                print('handled\t', reads_count * 2 // 1000000, 'm reads, ', round(_time_end_, 2), 's/m reads',
                      sep="", end='\r')
            else:
                print('INFO: handled {} M reads, {:.2f} s/M reads'.format(reads_count * 2 // 1000000, _time_end_))
    infile_1.close()
    infile_2.close()


# Used to increase the kmer for filtering the larger data after filtering
def re_filter_reads(_ref_kmer_dict_: dict, _kmer_size_: int, _step_size_: int, gene_name: str, _out_dir_: str,
                    _read_reverse_complement_=True) -> tuple:
    refilter_reads_count = 0
    refilter_reads_length = 0
    out_file_stream = open(os.path.join(_out_dir_, gene_name + ".fasta"), "w+")
    infile_stream = open(os.path.join(_out_dir_, "big_reads", gene_name + ".fasta"), 'r')
    for _ in infile_stream:
        read_seq = infile_stream.readline()
        filtered_flag = False
        for j in range(0, len(read_seq) - _kmer_size_, _step_size_):
            kmer = read_seq[j:j + _kmer_size_]
            if kmer in _ref_kmer_dict_:
                filtered_flag = True
                break
        # When the sequence is not filtered successfully and the read reverse complement needs to be processed
        if not filtered_flag and _read_reverse_complement_:
            temp_seq = reverse_complement_limit(read_seq)
            # have \n, the j+_kmer_size not -1
            for j in range(0, len(temp_seq) - _kmer_size_, _step_size_):
                kmer = temp_seq[j:j + _kmer_size_]
                if kmer in _ref_kmer_dict_:
                    filtered_flag = True
                    break
        if filtered_flag:
            refilter_reads_count += 1
            refilter_reads_length += len(read_seq)
            out_file_stream.writelines([_, read_seq])
    out_file_stream.close()
    infile_stream.close()
    return refilter_reads_count, refilter_reads_length


# re-filter one file
def refilter_one_gene(gene_name: str, gene_avg_len: int, _refilter_kmer_size_: int,
                      _reference_path_: str, _out_dir_: str, _refilter_step_size_: int = 1):
    _refilter_gene_info_dict_ = collections.defaultdict(dict)
    _refilter_gene_info_dict_[gene_name]["ref_avg_length"] = gene_avg_len
    while True:
        print("re-filter:", gene_name, 'with k =', _refilter_kmer_size_)
        # if not reverse and not complement reference sequences. reverse and complement the reads, vice versa.
        ref_kmer_dict = make_ref_kmer_dict(_reference_path_, _refilter_kmer_size_, _ref_reverse_complement_=True,
                                           _print_=False)
        filter_reads_count, filter_reads_length = re_filter_reads(ref_kmer_dict, _refilter_kmer_size_,
                                                                  _refilter_step_size_, gene_name,
                                                                  _out_dir_, _read_reverse_complement_=False)

        _refilter_gene_info_dict_[gene_name]["filter_reads_count"] = filter_reads_count
        _refilter_gene_info_dict_[gene_name]["filter_reads_length"] = filter_reads_length
        _refilter_gene_info_dict_[gene_name]["filter_kmer"] = _refilter_kmer_size_

        if filter_reads_length / gene_avg_len < 512 or get_file_size(
                os.path.join(_out_dir_, gene_name + ".fasta")) < 8 or _refilter_kmer_size_ >= 75:
            break
        else:
            if os.path.isfile(os.path.join(_out_dir_, "big_reads", gene_name + ".fasta")):
                os.remove(os.path.join(_out_dir_, "big_reads", gene_name + ".fasta"))
            shutil.move(os.path.join(_out_dir_, gene_name + ".fasta"),
                        os.path.join(_out_dir_, 'big_reads', gene_name + ".fasta"))
            _refilter_kmer_size_ += 2
    return _refilter_gene_info_dict_


def filter_flow(_read_data_tuple_: tuple, _out_dir_: str, _reference_path_: str, _kmer_size_: int,
                _step_size_: int = 1, _ref_reverse_complement_: bool = False,
                _read_reverse_complement_: bool = False, refilter: bool = True,
                _clear_: bool = True, _pos_: bool = True, _paired_reads_: bool = True,
                _thread_for_filter_: int = 4, _print_: bool = True, _ref_number_: int = None):
    if not os.path.isdir(_out_dir_):
        os.makedirs(_out_dir_)
    print("Getting information from references")
    # if not reverse and complement the reference sequences, reverse and complement the reads
    if not _ref_reverse_complement_:
        _read_reverse_complement_ = True

    _time_build_hash_start_ = time.time()
    # ref_length_dict: Record the average length of the read of the reference file
    # ref_path_dict: Record the path of the reference file
    ref_length_dict, ref_path_dict = get_ref_info(_reference_path_)
    # build hash table
    print("Building hash table")

    # Process the reference sequences and generate the specific kmers stored in ref_kmer_dict
    # ref_kmer_dict: key->kmer_seq value->list
    # list[0] Number of occurrences of kmer
    # list[1] the sum of the percentage positions of kmer appearing on a reference gene
    # list[2:] the genes in which the kmer appears
    ref_kmer_dict = make_ref_kmer_dict(_reference_path_, _kmer_size_, _ref_reverse_complement_, _pos_, _print_,
                                       _ref_number_)
    print("Hash dictionary has been made")
    _time_build_hash_end = time.time()
    print('Time used for building hash table: {:.2f} s'.format(_time_build_hash_end - _time_build_hash_start_))

    # _clear_ is used to delete the original file with the same name in the output folder and create a new file.
    if _clear_:
        for key in ref_path_dict:
            with open(os.path.join(_out_dir_, key + ".fasta"), 'w'):
                pass
    print("INFO: The memory usage of the current process is: {:.2f} GB".
          format(psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024))
    print('INFO: Filter reads from fq_files based on hash table with {} thread'.format(_thread_for_filter_))

    _time_filter_start_ = time.time()
    # do read filtering
    # unpaired-end data
    if not _paired_reads_:
        _unpaired_file_list_ = _read_data_tuple_[0]
        if _thread_for_filter_ == 1:
            for unpaired_file in _unpaired_file_list_:
                do_reads_filter(ref_kmer_dict, _kmer_size_, _step_size_, unpaired_file,
                                _out_dir_, _read_reverse_complement_, _print_)
        else:
            pool = multiprocessing.Pool(min(len(_unpaired_file_list_), _thread_for_filter_))
            for unpaired_file in _unpaired_file_list_:
                pool.apply_async(do_reads_filter, args=(ref_kmer_dict, _kmer_size_, _step_size_, unpaired_file,
                                                        _out_dir_, _read_reverse_complement_, _print_), )
            pool.close()
            pool.join()
    # paired-end data
    if _paired_reads_:
        paired_file_list_one = _read_data_tuple_[0]
        paired_file_list_two = _read_data_tuple_[1]
        if len(paired_file_list_one) == len(paired_file_list_two):
            for i in range(len(paired_file_list_one)):
                if _thread_for_filter_ == 1:
                    # when filter_thread is 1
                    do_pair_reads_filter(ref_kmer_dict, _kmer_size_, _step_size_,
                                         paired_file_list_one[i], paired_file_list_two[i],
                                         _out_dir_, 0, 1, _read_reverse_complement_, _print_)
                else:
                    # create the multiprocessing pool
                    pool = multiprocessing.Pool(_thread_for_filter_)
                    # use  multiprocessing to read one file
                    for j in range(_thread_for_filter_):
                        pool.apply_async(func=do_pair_reads_filter,
                                         args=(ref_kmer_dict, _kmer_size_, _step_size_,
                                               paired_file_list_one[i], paired_file_list_two[i],
                                               _out_dir_, j, _thread_for_filter_, _read_reverse_complement_, _print_,))
                    pool.close()
                    pool.join()
        else:
            print("The number of paired_end fastq files is not equal")
            exit(0)
    _time_filter_end_ = time.time()
    print('Time used for filter: {:.2f} s'.format(_time_filter_end_ - _time_filter_start_))
    # clean memory manually
    del ref_kmer_dict
    gc.collect()

    # Get the number of filtered reads and the total length of reads based on the filtered data
    filter_gene_info_dict = collections.defaultdict(dict)
    for gene_name, ref_avg_length in ref_length_dict.items():
        filter_gene_info_dict[gene_name]["ref_avg_length"] = ref_avg_length
        filter_gene_info_dict[gene_name]["filter_kmer"] = _kmer_size_
        filter_gene_info_dict[gene_name]["filter_reads_count"], filter_gene_info_dict[gene_name][
            "filter_reads_length"] = get_reads_info(os.path.join(_out_dir_, gene_name + ".fasta"))
    if refilter:
        # Record the genes to be filtered  key:gene_name value:[avg_len,gene_path]
        refilter_gene_info_dict = collections.defaultdict(dict)
        if not os.path.isdir(os.path.join(_out_dir_, 'big_reads')):
            os.makedirs(os.path.join(_out_dir_, 'big_reads'))
        # ref_length_dict: record the average length of the reads in the reference files
        for gene_name, gene_info in filter_gene_info_dict.items():
            if os.path.isfile(os.path.join(_out_dir_, gene_name + ".fasta")):
                # if coverage > ~512 or the file size > 8M re-filter
                # coverage: c = LN / G
                if (gene_info["filter_reads_length"] / gene_info.get("ref_avg_length") > 512) \
                        and get_file_size(os.path.join(_out_dir_, gene_name + ".fasta")) > 8:
                    # Move the files to be filtered into a folder
                    shutil.move(os.path.join(_out_dir_, gene_name + ".fasta"),
                                os.path.join(_out_dir_, 'big_reads', gene_name + ".fasta"))
                    refilter_gene_info_dict[gene_name] = gene_info
        # clean memory
        gc.collect()
        if not refilter_gene_info_dict:
            print("Big reads files don't exist, no need to reFilter!")
        else:
            print("reFilter")
            with ThreadPoolExecutor(max_workers=min(_thread_for_filter_, len(refilter_gene_info_dict))) as executor:
                futures = [executor.submit(
                    partial(refilter_one_gene, _refilter_kmer_size_=_kmer_size_ + 2, _out_dir_=_out_dir_,
                            _refilter_step_size_=1), gene_name=gene_name, gene_avg_len=gene_info["ref_avg_length"],
                    _reference_path_=ref_path_dict[gene_name])
                    for gene_name, gene_info in refilter_gene_info_dict.items()]
                for future in as_completed(futures):
                    refilter_gene_info_dict.update(future.result())
        # log
        filter_gene_info_dict.update(refilter_gene_info_dict)
        log_file = os.path.join(_out_dir_, "filter_log.csv")
        log(log_file, "gene_id", "ref_avg_length", "filter_reads_count", "filter_kmer")
        for gene_name, gene_info in filter_gene_info_dict.items():
            log(log_file, gene_name, gene_info["ref_avg_length"], gene_info["filter_reads_count"],
                gene_info["filter_kmer"])
        # Return the filtered gene information
        return filter_gene_info_dict


if __name__ == "__main__":
    pars = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, usage="%(prog)s [options]",
                                   description="Easy353 zzhen@sculab")
    pars.add_argument("-1", dest="fq_file_1", type=str, nargs="+",
                      help="Input file(s) with forward paired-end reads (*.fq/.gz/.tar.gz).", required=False)
    pars.add_argument("-2", dest="fq_file_2", type=str, nargs="+",
                      help="Input file(s) with reverse paired-end reads (*.fq/.gz/.tar.gz).", required=False)
    pars.add_argument("-u", dest="unpaired_fq_file", type=str,
                      help="Input file(s) with unpaired (single-end) reads.", required=False, nargs="+")
    pars.add_argument("-r", dest="reference", type=str, help="Input a file(directory) with references", required=True)
    pars.add_argument("-o", dest="output_dir", type=str, help="Output directory.", required=False,
                      default="easy353_output")
    pars.add_argument("-k", dest="filter_kmer", type=int, help="Kmer setting for filtering reads. Default:31",
                      default=31)
    pars.add_argument("-s", dest="step_length", type=int,
                      help="Step length of the sliding window on the reads. Default:1", default=1)
    pars.add_argument("-t", dest="filter_thread", type=int,
                      help="Threads setting for filtering reads. Default:4", default=4)
    pars.add_argument("-fast", dest="fast", action="store_true", help="Whether to use fast mode.")
    # 默认限制使用的参考序列的数量
    pars.add_argument("-reference_number", dest="reference_number", type=int,
                      help="The number of the reference sequences used to build hash table. Default:all", default=None)
    args = pars.parse_args()

    fastq_files = tuple()
    _paired_reads_ = False
    if args.unpaired_fq_file:
        fastq_files = (args.unpaired_fq_file,)
    if args.fq_file_1 and args.fq_file_2:
        fastq_files = (args.fq_file_1, args.fq_file_2)
        _paired_reads_ = True

    #  set the start method of multiprocessing
    if platform.system() in ("Linux", "Darwin"):
        multiprocessing.set_start_method('fork')
    else:
        multiprocessing.set_start_method('spawn')
    filter_flow(_read_data_tuple_=fastq_files, _out_dir_=args.output_dir,
                _reference_path_=args.reference, _kmer_size_=args.filter_kmer, _step_size_=args.step_length,
                _ref_reverse_complement_=args.fast, _paired_reads_=_paired_reads_,
                _thread_for_filter_=args.filter_thread, _ref_number_=args.reference_number)
